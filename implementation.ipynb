{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1aecb6c-4de9-444c-920b-a48dba2b3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "import kagglehub\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import sentencepiece as spm\n",
    "import random\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58e0d7-e192-47ff-8e65-e4778dd18cd7",
   "metadata": {},
   "source": [
    "# Creating corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5119aa9d-67c3-4134-9ac4-289dd25b416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"corpus/\"\n",
    "english_files = [\"commoncrawl.de-en.en\", \"europarl-v7.de-en.en\"]\n",
    "german_files = [\"commoncrawl.de-en.de\", \"europarl-v7.de-en.de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95fb6ab3-3a56-418f-8be7-6657758a2fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2399123\n",
      "2399123\n",
      "4319332\n",
      "4319332\n"
     ]
    }
   ],
   "source": [
    "english_sentences = []\n",
    "german_sentences = []\n",
    "\n",
    "for i in range(len(english_files)):\n",
    "    with open(corpus_path + english_files[i], 'r', encoding='utf-8') as f:\n",
    "        english_sentences.extend(f.read().splitlines())\n",
    "    print(len(english_sentences))\n",
    "    \n",
    "    with open(corpus_path + german_files[i], 'r', encoding='utf-8') as f:\n",
    "        german_sentences.extend(f.read().splitlines())\n",
    "    print(len(german_sentences))\n",
    "\n",
    "if len(english_sentences) != len(german_sentences):\n",
    "    raise ValueError(\"Mismatch in number of sentences between English and German files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "652ce6d4-2e1c-4bc7-affb-aa4d240ba6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with mistmatched number of lines: []\n"
     ]
    }
   ],
   "source": [
    "# News commentary are misaligned and therefore a different approach is taken\n",
    "english_folder = corpus_path + \"news_commentary/English\"\n",
    "german_folder = corpus_path + \"news_commentary/German\"\n",
    "\n",
    "english_files = sorted(os.listdir(english_folder))\n",
    "german_files = sorted(os.listdir(german_folder))\n",
    "\n",
    "if len(english_files) != len(german_files):\n",
    "    raise ValueError(\"Mismatch in number of files between English and German folders for news commentary.\")\n",
    "\n",
    "mismatched_files = []\n",
    "\n",
    "for eng_file, ger_file in zip(english_files, german_files):\n",
    "    eng_path = os.path.join(english_folder, eng_file)\n",
    "    ger_path = os.path.join(german_folder, ger_file)\n",
    "\n",
    "    # Ensuring same name for each file tuple\n",
    "    if os.path.splitext(eng_file)[0] != os.path.splitext(ger_file)[0]:\n",
    "        raise ValueError(f\"File mismatch: {eng_file} and {ger_file} do not correspond.\")\n",
    "\n",
    "    with open(eng_path, 'r', encoding='utf-8') as f:\n",
    "        english_section = [segment.strip() for segment in f.read().split('<P>') if segment.strip()]\n",
    "\n",
    "    with open(ger_path, 'r', encoding='utf-8') as f:\n",
    "        german_section = [segment.strip() for segment in f.read().split('<P>') if segment.strip()]\n",
    "\n",
    "    if len(english_section) != len(german_section):\n",
    "        mismatched_files.append((eng_file, ger_file, len(english_section), len(german_section)))\n",
    "    else:\n",
    "        # If line counts match, add sentences to the lists\n",
    "        english_sentences.extend(english_section)\n",
    "        german_sentences.extend(german_section)\n",
    "\n",
    "print(f\"Files with mistmatched number of lines: {mismatched_files}\")\n",
    "\n",
    "if len(english_sentences) != len(german_sentences):\n",
    "    raise ValueError(\"Mismatch in number of sentences between English and German files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70313925-6600-423b-b98b-564c08609d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All together there are 4394358 english sentences in the corpus.\n",
      "All together there are 4394358 german sentences in the corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"All together there are {len(english_sentences)} english sentences in the corpus.\")\n",
    "print(f\"All together there are {len(german_sentences)} german sentences in the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb38b04-2acc-4819-a2f0-48e2fa10a89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges (corners) of the steel ingot mould.</td>\n",
       "      <td>iron cement ist eine gebrauchs-fertige Paste, die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken (Winkel) der Stahlguss -Kokille aufgetragen wird.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iron cement protects the ingot against the hot, abrasive steel casting process.</td>\n",
       "      <td>Nach der Aushärtung schützt iron cement die Kokille gegen den heissen, abrasiven Stahlguss .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fire restant repair cement for fire places, ovens, open fireplaces etc.</td>\n",
       "      <td>feuerfester Reparaturkitt für Feuerungsanlagen, Öfen, offene Feuerstellen etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Construction and repair of highways and...</td>\n",
       "      <td>Der Bau und die Reparatur der Autostraßen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An announcement must be commercial character.</td>\n",
       "      <td>die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394353</th>\n",
       "      <td>The stakes for Africa are enormous.\\nSouth Africa has the continent’s largest economy and, until the global financial crisis, posted 10 years of steady economic growth.\\nIn an economic slowdown, the country’s severe crime problem might only worsen; so might unemployment, which already tops 20% in the formal economy.</td>\n",
       "      <td>Für Afrika steht Enormes auf dem Spiel.\\nSüdafrika ist die größte Ökonomie des Kontinents und bis zur globalen Finanzkrise erlebte man 10 Jahre beständigen Wirtschaftswachstums.\\nIn Zeiten des Abschwungs kann sich das immense Kriminalitätsproblem des Landes nur verschärfen. Das gilt auch für die Arbeitslosigkeit, die im Bereich der offiziellen Wirtschaft bereits über 20 Prozent liegt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394354</th>\n",
       "      <td>Zuma senses the urgency of the situation.\\nHe is, after all, 67 years old and likely to serve only a single term in office. “We can’t waste time,” he says.</td>\n",
       "      <td>Zuma weiß um die Dringlichkeit der Situation.\\nImmerhin ist er 67 Jahre alt und wird wahrscheinlich nur eine Amtszeit dienen. „Wir können uns keine Zeitverschwendung leisten“, sagt er.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394355</th>\n",
       "      <td>Yet, according to the political economist Moeletsi Mbeki, at his core, “Zuma is a conservative.” In this sense, Zuma represents yesterday’s South Africa.\\nHe is part of the proud generation that defeated apartheid – and then peacefully engineered a transition to durable black-majority rule.\\nTheir achievement remains one of the greatest in recent history.</td>\n",
       "      <td>Dem politischen Ökonomen Moeletsi Mbeki zufolge, ist Zuma im Grunde seines Herzens „ein Konservativer“.  In diesem Sinne vertritt Zuma das Südafrika von gestern.\\nEr ist Mitglied einer stolzen Generation, die die Apartheid bezwang – und der anschließend ein friedlicher Übergang zu einer schwarzen Mehrheitsregierung gelang.\\nDas bleibt eine der größten Errungenschaften in der jüngeren Geschichte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394356</th>\n",
       "      <td>At the same time, Zuma’s revolutionary generation still seems uneasy leading South Africa in a post-apartheid era that is now 15 years old.\\nIn a region that reveres the elderly, Zuma’s attachment to his rural traditions must be matched by an equal openness to the appetites of the country’s youth.</td>\n",
       "      <td>Gleichzeitig scheint sich Zumas revolutionäre Generation mit der Führung Südafrikas in der nun seit 15 Jahren dauernden Ära nach der Apartheid noch immer unwohl zu fühlen.\\nIn einer Region, wo die älteren Menschen sehr verehrt werden, muss Zumas Bindung an landestypische Traditionen eine gleichwertige Offenheit gegenüber den Bedürfnissen der Jugend des Landes gegenüberstehen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394357</th>\n",
       "      <td>Three in ten South Africans are younger than 15, meaning that they did not live a day under apartheid.\\nSomehow Zuma must find a way to honor his own generation’s commitment to racial justice and national liberation, while empowering the masses who daily suffer the sting of class differences and yearn for material gain.</td>\n",
       "      <td>Drei von zehn Südafrikanern sind jünger als 15 und das bedeutet, dass sie nicht einen Tag unter der Apartheid gelebt haben.\\nIrgendwie muss Zuma einen Weg finden, einerseits das Engagement seiner Generation hinsichtlich ethnischer Gerechtigkeit und nationaler Befreiung zu würdigen und andererseits den Massen, die täglich unter Klassenunterschieden leiden und sich nach materiellen Verbesserungen sehnen, mehr Mitwirkungsmöglichkeiten einzuräumen.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4394358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                            en  \\\n",
       "0                                                                                                                                                                                                                               iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges (corners) of the steel ingot mould.   \n",
       "1                                                                                                                                                                                                                                                                                              iron cement protects the ingot against the hot, abrasive steel casting process.   \n",
       "2                                                                                                                                                                                                                                                                                                    a fire restant repair cement for fire places, ovens, open fireplaces etc.   \n",
       "3                                                                                                                                                                                                                                                                                                                                   Construction and repair of highways and...   \n",
       "4                                                                                                                                                                                                                                                                                                                                An announcement must be commercial character.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "4394353                                          The stakes for Africa are enormous.\\nSouth Africa has the continent’s largest economy and, until the global financial crisis, posted 10 years of steady economic growth.\\nIn an economic slowdown, the country’s severe crime problem might only worsen; so might unemployment, which already tops 20% in the formal economy.   \n",
       "4394354                                                                                                                                                                                                            Zuma senses the urgency of the situation.\\nHe is, after all, 67 years old and likely to serve only a single term in office. “We can’t waste time,” he says.   \n",
       "4394355  Yet, according to the political economist Moeletsi Mbeki, at his core, “Zuma is a conservative.” In this sense, Zuma represents yesterday’s South Africa.\\nHe is part of the proud generation that defeated apartheid – and then peacefully engineered a transition to durable black-majority rule.\\nTheir achievement remains one of the greatest in recent history.   \n",
       "4394356                                                             At the same time, Zuma’s revolutionary generation still seems uneasy leading South Africa in a post-apartheid era that is now 15 years old.\\nIn a region that reveres the elderly, Zuma’s attachment to his rural traditions must be matched by an equal openness to the appetites of the country’s youth.   \n",
       "4394357                                      Three in ten South Africans are younger than 15, meaning that they did not live a day under apartheid.\\nSomehow Zuma must find a way to honor his own generation’s commitment to racial justice and national liberation, while empowering the masses who daily suffer the sting of class differences and yearn for material gain.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                       de  \n",
       "0                                                                                                                                                                                                                                                                                                  iron cement ist eine gebrauchs-fertige Paste, die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken (Winkel) der Stahlguss -Kokille aufgetragen wird.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                            Nach der Aushärtung schützt iron cement die Kokille gegen den heissen, abrasiven Stahlguss .  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                          feuerfester Reparaturkitt für Feuerungsanlagen, Öfen, offene Feuerstellen etc.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                            Der Bau und die Reparatur der Autostraßen...  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                              die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen.  \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  \n",
       "4394353                                                               Für Afrika steht Enormes auf dem Spiel.\\nSüdafrika ist die größte Ökonomie des Kontinents und bis zur globalen Finanzkrise erlebte man 10 Jahre beständigen Wirtschaftswachstums.\\nIn Zeiten des Abschwungs kann sich das immense Kriminalitätsproblem des Landes nur verschärfen. Das gilt auch für die Arbeitslosigkeit, die im Bereich der offiziellen Wirtschaft bereits über 20 Prozent liegt.  \n",
       "4394354                                                                                                                                                                                                                                                                          Zuma weiß um die Dringlichkeit der Situation.\\nImmerhin ist er 67 Jahre alt und wird wahrscheinlich nur eine Amtszeit dienen. „Wir können uns keine Zeitverschwendung leisten“, sagt er.  \n",
       "4394355                                                    Dem politischen Ökonomen Moeletsi Mbeki zufolge, ist Zuma im Grunde seines Herzens „ein Konservativer“.  In diesem Sinne vertritt Zuma das Südafrika von gestern.\\nEr ist Mitglied einer stolzen Generation, die die Apartheid bezwang – und der anschließend ein friedlicher Übergang zu einer schwarzen Mehrheitsregierung gelang.\\nDas bleibt eine der größten Errungenschaften in der jüngeren Geschichte.  \n",
       "4394356                                                                        Gleichzeitig scheint sich Zumas revolutionäre Generation mit der Führung Südafrikas in der nun seit 15 Jahren dauernden Ära nach der Apartheid noch immer unwohl zu fühlen.\\nIn einer Region, wo die älteren Menschen sehr verehrt werden, muss Zumas Bindung an landestypische Traditionen eine gleichwertige Offenheit gegenüber den Bedürfnissen der Jugend des Landes gegenüberstehen.  \n",
       "4394357  Drei von zehn Südafrikanern sind jünger als 15 und das bedeutet, dass sie nicht einen Tag unter der Apartheid gelebt haben.\\nIrgendwie muss Zuma einen Weg finden, einerseits das Engagement seiner Generation hinsichtlich ethnischer Gerechtigkeit und nationaler Befreiung zu würdigen und andererseits den Massen, die täglich unter Klassenunterschieden leiden und sich nach materiellen Verbesserungen sehnen, mehr Mitwirkungsmöglichkeiten einzuräumen.  \n",
       "\n",
       "[4394358 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'en': english_sentences, 'de': german_sentences}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb5373-cefd-4b0d-add2-7824bfcf551c",
   "metadata": {},
   "source": [
    "# Normalizing the corpus\n",
    "- removing newlines large texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50256197-ac37-43e6-9f5a-ce0472b11225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "        # Converting to lowercase\n",
    "        #text = text.lower()\n",
    "    \n",
    "        # Normalizing unicode chars\n",
    "        #text = unicodedata.normalize('NFKC', text)\n",
    "        \n",
    "        # Remove leading/trailing and extra spaces\n",
    "        #text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "        # Add spaces around punctuation\n",
    "        #text = re.sub(r'([.,!?;:()\\-])', r' \\1 ', text)\n",
    "    \n",
    "        # Replace newline characters with a placeholder\n",
    "        text = text.replace('\\n', ' ')\n",
    "    \n",
    "        # Remove non-textual artifacts (allow only alphanumerics and selected punctuations)\n",
    "        #text = re.sub(r'[^a-zA-Z0-9.,!?;:()\\- ]', '', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7befd81d-e9a0-41a2-b889-67609328c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df.copy(deep=True)\n",
    "df_normalized['en'] = df_normalized['en'].apply(normalize_text)\n",
    "df_normalized['de'] = df_normalized['de'].apply(normalize_text)\n",
    "df_normalized.to_csv(\"df_normalized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "950ac37e-bcd2-485a-85c7-1934107db296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en                                                                          The stakes for Africa are enormous. South Africa has the continent’s largest economy and, until the global financial crisis, posted 10 years of steady economic growth. In an economic slowdown, the country’s severe crime problem might only worsen; so might unemployment, which already tops 20% in the formal economy.\n",
       "de    Für Afrika steht Enormes auf dem Spiel. Südafrika ist die größte Ökonomie des Kontinents und bis zur globalen Finanzkrise erlebte man 10 Jahre beständigen Wirtschaftswachstums. In Zeiten des Abschwungs kann sich das immense Kriminalitätsproblem des Landes nur verschärfen. Das gilt auch für die Arbeitslosigkeit, die im Bereich der offiziellen Wirtschaft bereits über 20 Prozent liegt.\n",
       "Name: 4394353, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_normalized.iloc[4394353])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17710527-c83e-4647-a269-ca78c15291e3",
   "metadata": {},
   "source": [
    "# Saving corpus for BPE model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ac35c5-5a99-418e-954b-713f8d32da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_normalized['en_length'] = df_normalized['en'].apply(lambda x: len(x.split()))\n",
    "#df_normalized['de_length'] = df_normalized['de'].apply(lambda x: len(x.split()))\n",
    "#df_normalized['total_length'] = df_normalized['en_length'] + df_normalized['de_length']\n",
    "#df_normalized['en_tokenized'] = df_normalized['en'].apply(lambda x: list(x))\n",
    "#df_normalized['de_tokenized'] = df_normalized['de'].apply(lambda x: list(x))\n",
    "\n",
    "# display(df_normalized)\n",
    "\n",
    "# To create corpus text file\n",
    "# corpus_df = pd.concat([df_normalized['en'], df_normalized['de']])\n",
    "\n",
    "# corpus_df.to_csv(\"corpus.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c209549f-73a4-4bec-b78e-16cdd602c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17b7500a-8fba-4014-97e4-dd31dff88cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.read_csv(\"df_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3250795-44f5-4091-94d8-671b03b23978",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_model.model')\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "sb_vocab = [sp.id_to_piece(i) for i in range(vocab_size)]\n",
    "print(sb_vocab[:10])\n",
    "sb_vocab_dict = {sb_vocab[i]: i for i in range(vocab_size)}\n",
    "print(sb_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17d90435-b8d0-4c0a-b7ba-ab3fd533cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '—', 'Ich', '▁f', 'ahre', '▁manchmal', '▁nach', '▁meine', '▁Familie', ',', '▁sie', '▁sind', '▁v', 'irk', 'lich', '▁überrascht', '.']\n",
      "[1376, 33, 1742, 10181, 459, 1960, 5773, 36926, 416, 321, 55, 11399, 116, 18093, 36927]\n",
      "Ich fahre manchmal nach meine Familie, sie sind virklich überrascht.\n",
      "form f pers Fraktionen nach day Conventionö sie sind v liefernlichzweifL\n"
     ]
    }
   ],
   "source": [
    "# Tokenize using SentencePiece\n",
    "tokens = sp.encode_as_pieces(\"—Ich fahre manchmal nach meine Familie, sie sind virklich überrascht.\")\n",
    "print(tokens)\n",
    "tokens = sp.encode(\"Ich fahre manchmal nach meine Familie, sie sind virklich überrascht.\")\n",
    "print(tokens)\n",
    "tokens = sp.decode_pieces(['▁Ich', '▁f', 'ahre', '▁manchmal', '▁nach', '▁meine', '▁Familie', ',', '▁sie', '▁sind', '▁v', 'irk', 'lich', '▁überrascht', '.'])\n",
    "print(tokens)\n",
    "tokens = sp.decode([1361, 33, 1724, 10072, 459, 1938, 5720, 36953, 416, 321, 55, 11279, 116, 17860, 36954])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa46b597-5222-4667-9bc7-60d3de875491",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'—'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msb_vocab_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m—\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '—'"
     ]
    }
   ],
   "source": [
    "print(sb_vocab_dict['—'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98c30143-f53b-4900-a9d5-ad19a286057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[▁iron, ▁c, ement, ▁is, ▁a, ▁ready, ▁for, ▁use, ▁paste, ▁which, ▁is, ▁laid, ▁as, ▁a, ▁fil, let, ▁by, ▁put, ty, ▁kn, ife, ▁or, ▁finger, ▁in, ▁the, ▁mould, ▁edges, ▁(, c, orn, ers, ), ▁of, ▁the, ▁steel, ▁ing, ot, ▁mould, .]</td>\n",
       "      <td>[▁iron, ▁c, ement, ▁ist, ▁eine, ▁gebrauch, s, -, fert, ige, ▁P, aste, ,, ▁die, ▁mit, ▁einem, ▁Sp, ach, tel, ▁oder, ▁den, ▁F, ing, ern, ▁als, ▁H, ohl, ke, hle, ▁in, ▁die, ▁For, me, cken, ▁(, W, inkel, ), ▁der, ▁Stahl, g, uss, ▁-, K, ok, ille, ▁aufge, tragen, ▁wird, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[▁iron, ▁c, ement, ▁protects, ▁the, ▁ing, ot, ▁against, ▁the, ▁hot, ,, ▁ab, ras, ive, ▁steel, ▁casting, ▁process, .]</td>\n",
       "      <td>[▁Nach, ▁der, ▁Aus, här, tung, ▁schützt, ▁iron, ▁c, ement, ▁die, ▁Kok, ille, ▁gegen, ▁den, ▁he, issen, ,, ▁ab, ras, iven, ▁Stahl, g, uss, ▁.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[▁a, ▁fire, ▁rest, ant, ▁repair, ▁c, ement, ▁for, ▁fire, ▁places, ,, ▁o, vens, ,, ▁open, ▁fire, places, ▁etc, .]</td>\n",
       "      <td>[▁fe, uer, f, ester, ▁Reparatur, k, itt, ▁für, ▁Feuer, ungs, anlagen, ,, ▁Ö, fen, ,, ▁offene, ▁Fe, u, erstellen, ▁etc, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[▁Construction, ▁and, ▁repair, ▁of, ▁high, ways, ▁and, ...]</td>\n",
       "      <td>[▁Der, ▁Bau, ▁und, ▁die, ▁Reparatur, ▁der, ▁Aut, ost, ra, ßen, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[▁An, ▁announcement, ▁must, ▁be, ▁commercial, ▁character, .]</td>\n",
       "      <td>[▁die, ▁Mitteilungen, ▁sollen, ▁den, ▁geschäftlichen, ▁kommerziellen, ▁Charakter, ▁tragen, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[▁Est, ablish, ed, ▁in, ▁1990,, ▁the, ▁office, ▁of, ▁Ha, ide, g, ger, ▁&amp;, ▁Partner, ▁in, ▁Budapest, ▁has, ▁been, ▁providing, ▁a, ▁full, ▁range, ▁of, ▁legal, ▁services, ▁offering, ▁individual, ▁tailored, ▁advice, .]</td>\n",
       "      <td>[▁Die, ▁im, ▁Jahre, ▁1990, ▁gegründete, ▁K, anzlei, ▁Ha, ide, g, ger, ▁&amp;, ▁Partner, ▁bietet, ▁eine, ▁alle, ▁Rechts, gebiete, ▁umfassende, ,, ▁auf, ▁den, ▁individuellen, ▁Bedarf, ▁zugeschnitten, e, ▁Rechts, beratung, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[▁Apart, ▁from, ▁being, ▁Hungary, ’, s, ▁principal, ▁political, ,, ▁commercial, ,, ▁industrial, ▁and, ▁transportation, ▁centre, ,, ▁the, ▁city, ▁of, ▁Budapest, ▁boasts, ▁sites, ,, ▁monuments, ▁and, ▁sp, as, ▁of, ▁worldwide, ▁ren, own, .]</td>\n",
       "      <td>[▁Budapest, ▁ist, ▁nicht, ▁nur, ▁das, ▁politische, ,, ▁wirtschaftliche, ,, ▁industrielle, ▁und, ▁verkehr, stechn, ische, ▁Herz, ▁Ungarn, s, ,, ▁sondern, ▁r, ühm, t, ▁sich, ▁auch, ▁weltweit, ▁bekannter, ▁Sehens, würdigkeiten, ,, ▁Denkm, äler, ▁und, ▁Bäder, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[▁This, ▁statistic, ▁is, ▁based, ▁on, ▁the, ▁68, 19, ▁using, ▁ecommerce, ▁sites, ▁(, esh, ops, ,, ▁distributors, ,, ▁comparison, ▁sites, ,, ▁ecommerce, ▁ASPs, ,, ▁purchase, ▁systems, ,, ▁etc, ), ▁downloading, ▁this, ▁ICEcat, ▁data, -, sheet, ▁since, ▁19, ▁Oct, ▁2007.]</td>\n",
       "      <td>[▁Diese, ▁Statistik, ▁basiert, ▁auf, ▁den, ▁teilnehmenden, ▁E, -, C, ommer, ces, eiten, ▁68, 19, ▁(, E, -, Shops, ,, ▁Distrib, ut, oren, ,, ▁Vergleich, ss, eiten, ,, ▁E, -, Commerce, ▁ASPs, ,, ▁Einkauf, ssysteme, ▁etc, ),, ▁welche, ▁dieses, ▁ICEcat, ▁Daten, blatt, ▁täglich, ▁seit, ▁dem, ▁21., ▁März, ▁2009, ▁herunterladen, ., ▁19, ▁Okt, ▁2007.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[▁Only, ▁spons, oring, ▁brands, ▁are, ▁included, ▁in, ▁the, ▁free, ▁Open, ▁ICEcat, ▁content, ▁distribution, ▁as, ▁used, ▁by, ▁63, 28, ▁free, ▁Open, ▁ICEcat, ▁users, ▁.]</td>\n",
       "      <td>[▁Nur, ▁Spons, orm, ark, en, ▁sind, ▁in, ▁der, ▁kostenfreien, ▁Open, ▁ICEcat, ▁Content, ▁Verteilung, ▁vertreten, ▁63, 28, ▁und, ▁werden, ▁von, ▁Open, ▁ICEcat, ▁Nutzern, ▁genutzt, ..]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[▁A, cer, ▁project, or, ▁accessories, ▁can, ▁optimize, ▁your, ▁A, cer, ▁X, 11, 60, /, X, 12, 60, ▁project, ors, ▁and, ▁expand, ▁the, ▁usage, ▁and, ▁mobility, ▁of, ▁your, ▁product, .]</td>\n",
       "      <td>[▁Das, ▁A, cer, ▁Projekt, or, z, ubehör, ▁kann, ▁die, ▁Nutz, barkeit, ▁und, ▁die, ▁Mobilität, ▁Ihres, ▁A, cer, ▁X, 11, 60, ▁/, ▁X, 12, 60, ▁Projekt, ors, ▁optimieren, ▁und, ▁erweitern, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                              en  \\\n",
       "0                                                  [▁iron, ▁c, ement, ▁is, ▁a, ▁ready, ▁for, ▁use, ▁paste, ▁which, ▁is, ▁laid, ▁as, ▁a, ▁fil, let, ▁by, ▁put, ty, ▁kn, ife, ▁or, ▁finger, ▁in, ▁the, ▁mould, ▁edges, ▁(, c, orn, ers, ), ▁of, ▁the, ▁steel, ▁ing, ot, ▁mould, .]   \n",
       "1                                                                                                                                                           [▁iron, ▁c, ement, ▁protects, ▁the, ▁ing, ot, ▁against, ▁the, ▁hot, ,, ▁ab, ras, ive, ▁steel, ▁casting, ▁process, .]   \n",
       "2                                                                                                                                                               [▁a, ▁fire, ▁rest, ant, ▁repair, ▁c, ement, ▁for, ▁fire, ▁places, ,, ▁o, vens, ,, ▁open, ▁fire, places, ▁etc, .]   \n",
       "3                                                                                                                                                                                                                    [▁Construction, ▁and, ▁repair, ▁of, ▁high, ways, ▁and, ...]   \n",
       "4                                                                                                                                                                                                                   [▁An, ▁announcement, ▁must, ▁be, ▁commercial, ▁character, .]   \n",
       "..                                                                                                                                                                                                                                                                           ...   \n",
       "95                                                        [▁Est, ablish, ed, ▁in, ▁1990,, ▁the, ▁office, ▁of, ▁Ha, ide, g, ger, ▁&, ▁Partner, ▁in, ▁Budapest, ▁has, ▁been, ▁providing, ▁a, ▁full, ▁range, ▁of, ▁legal, ▁services, ▁offering, ▁individual, ▁tailored, ▁advice, .]   \n",
       "96                                 [▁Apart, ▁from, ▁being, ▁Hungary, ’, s, ▁principal, ▁political, ,, ▁commercial, ,, ▁industrial, ▁and, ▁transportation, ▁centre, ,, ▁the, ▁city, ▁of, ▁Budapest, ▁boasts, ▁sites, ,, ▁monuments, ▁and, ▁sp, as, ▁of, ▁worldwide, ▁ren, own, .]   \n",
       "97  [▁This, ▁statistic, ▁is, ▁based, ▁on, ▁the, ▁68, 19, ▁using, ▁ecommerce, ▁sites, ▁(, esh, ops, ,, ▁distributors, ,, ▁comparison, ▁sites, ,, ▁ecommerce, ▁ASPs, ,, ▁purchase, ▁systems, ,, ▁etc, ), ▁downloading, ▁this, ▁ICEcat, ▁data, -, sheet, ▁since, ▁19, ▁Oct, ▁2007.]   \n",
       "98                                                                                                      [▁Only, ▁spons, oring, ▁brands, ▁are, ▁included, ▁in, ▁the, ▁free, ▁Open, ▁ICEcat, ▁content, ▁distribution, ▁as, ▁used, ▁by, ▁63, 28, ▁free, ▁Open, ▁ICEcat, ▁users, ▁.]   \n",
       "99                                                                                        [▁A, cer, ▁project, or, ▁accessories, ▁can, ▁optimize, ▁your, ▁A, cer, ▁X, 11, 60, /, X, 12, 60, ▁project, ors, ▁and, ▁expand, ▁the, ▁usage, ▁and, ▁mobility, ▁of, ▁your, ▁product, .]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                           de  \n",
       "0                                                                                 [▁iron, ▁c, ement, ▁ist, ▁eine, ▁gebrauch, s, -, fert, ige, ▁P, aste, ,, ▁die, ▁mit, ▁einem, ▁Sp, ach, tel, ▁oder, ▁den, ▁F, ing, ern, ▁als, ▁H, ohl, ke, hle, ▁in, ▁die, ▁For, me, cken, ▁(, W, inkel, ), ▁der, ▁Stahl, g, uss, ▁-, K, ok, ille, ▁aufge, tragen, ▁wird, .]  \n",
       "1                                                                                                                                                                                                               [▁Nach, ▁der, ▁Aus, här, tung, ▁schützt, ▁iron, ▁c, ement, ▁die, ▁Kok, ille, ▁gegen, ▁den, ▁he, issen, ,, ▁ab, ras, iven, ▁Stahl, g, uss, ▁.]  \n",
       "2                                                                                                                                                                                                                                   [▁fe, uer, f, ester, ▁Reparatur, k, itt, ▁für, ▁Feuer, ungs, anlagen, ,, ▁Ö, fen, ,, ▁offene, ▁Fe, u, erstellen, ▁etc, .]  \n",
       "3                                                                                                                                                                                                                                                                                         [▁Der, ▁Bau, ▁und, ▁die, ▁Reparatur, ▁der, ▁Aut, ost, ra, ßen, ...]  \n",
       "4                                                                                                                                                                                                                                                               [▁die, ▁Mitteilungen, ▁sollen, ▁den, ▁geschäftlichen, ▁kommerziellen, ▁Charakter, ▁tragen, .]  \n",
       "..                                                                                                                                                                                                                                                                                                                                                        ...  \n",
       "95                                                                                                                                 [▁Die, ▁im, ▁Jahre, ▁1990, ▁gegründete, ▁K, anzlei, ▁Ha, ide, g, ger, ▁&, ▁Partner, ▁bietet, ▁eine, ▁alle, ▁Rechts, gebiete, ▁umfassende, ,, ▁auf, ▁den, ▁individuellen, ▁Bedarf, ▁zugeschnitten, e, ▁Rechts, beratung, .]  \n",
       "96                                                                                         [▁Budapest, ▁ist, ▁nicht, ▁nur, ▁das, ▁politische, ,, ▁wirtschaftliche, ,, ▁industrielle, ▁und, ▁verkehr, stechn, ische, ▁Herz, ▁Ungarn, s, ,, ▁sondern, ▁r, ühm, t, ▁sich, ▁auch, ▁weltweit, ▁bekannter, ▁Sehens, würdigkeiten, ,, ▁Denkm, äler, ▁und, ▁Bäder, .]  \n",
       "97  [▁Diese, ▁Statistik, ▁basiert, ▁auf, ▁den, ▁teilnehmenden, ▁E, -, C, ommer, ces, eiten, ▁68, 19, ▁(, E, -, Shops, ,, ▁Distrib, ut, oren, ,, ▁Vergleich, ss, eiten, ,, ▁E, -, Commerce, ▁ASPs, ,, ▁Einkauf, ssysteme, ▁etc, ),, ▁welche, ▁dieses, ▁ICEcat, ▁Daten, blatt, ▁täglich, ▁seit, ▁dem, ▁21., ▁März, ▁2009, ▁herunterladen, ., ▁19, ▁Okt, ▁2007.]  \n",
       "98                                                                                                                                                                     [▁Nur, ▁Spons, orm, ark, en, ▁sind, ▁in, ▁der, ▁kostenfreien, ▁Open, ▁ICEcat, ▁Content, ▁Verteilung, ▁vertreten, ▁63, 28, ▁und, ▁werden, ▁von, ▁Open, ▁ICEcat, ▁Nutzern, ▁genutzt, ..]  \n",
       "99                                                                                                                                                                [▁Das, ▁A, cer, ▁Projekt, or, z, ubehör, ▁kann, ▁die, ▁Nutz, barkeit, ▁und, ▁die, ▁Mobilität, ▁Ihres, ▁A, cer, ▁X, 11, 60, ▁/, ▁X, 12, 60, ▁Projekt, ors, ▁optimieren, ▁und, ▁erweitern, .]  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Function to encode a sentence\n",
    "def encode_sentence(sentence):\n",
    "    return sp.encode_as_pieces(sentence)\n",
    "\n",
    "# Apply the encoder to each column\n",
    "df_encoded = df_normalized.applymap(encode_sentence).copy(deep=True)\n",
    "\n",
    "# Save or inspect the new DataFrame\n",
    "display(df_encoded)\n",
    "print(type(df_encoded['en'][0]))\n",
    "#df_encoded.to_csv(\"df_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1606973c-b064-4211-969f-5af0b452cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, start_token=\"<s>\", end_token=\"</s>\", pad_token=\"<mask>\"):\n",
    "        \"\"\"\n",
    "        Initialize the translation dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: pandas DataFrame with 'en' and 'de' columns containing tokenized sentences\n",
    "            vocab: shared vocabulary from BPE model\n",
    "            start_token: token to mark the start of sentences\n",
    "            end_token: token to mark the end of sentences\n",
    "            pad_token: token used for padding\n",
    "        \"\"\"\n",
    "        self.en_sentences = dataframe['en'].tolist()\n",
    "        self.de_sentences = dataframe['de'].tolist()\n",
    "        self.vocab = vocab\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Add special tokens to both source and target sentences\n",
    "        en_tokens = [self.start_token] + self.en_sentences[idx] + [self.end_token]\n",
    "        de_tokens = [self.start_token] + self.de_sentences[idx] + [self.end_token]\n",
    "        \n",
    "        return {\n",
    "            'en': en_tokens,\n",
    "            'de': de_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0d017d3-8191-44d9-9b9d-b088c9a1863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, vocab):\n",
    "    \"\"\"\n",
    "    Custom collate function to create batches with padding.\n",
    "    \n",
    "    Args:\n",
    "        batch: list of dictionaries containing source and target tokens\n",
    "        vocab: shared vocabulary from BPE model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing padded and converted tensor sequences\n",
    "    \"\"\"\n",
    "    en_sequences = []\n",
    "    de_sequences = []\n",
    "    \n",
    "    for item in batch:\n",
    "        en_indices = torch.tensor([vocab.get(token, vocab['<unk>']) for token in item['en']])\n",
    "        de_indices = torch.tensor([vocab.get(token, vocab['<unk>']) for token in item['de']])\n",
    "        \n",
    "        en_sequences.append(en_indices)\n",
    "        de_sequences.append(de_indices)\n",
    "    \n",
    "    # Pad sequences to the longest sequence in the batch\n",
    "    en_padded = pad_sequence(en_sequences, batch_first=True, padding_value=vocab['<mask>'])\n",
    "    de_padded = pad_sequence(de_sequences, batch_first=True, padding_value=vocab['<mask>'])\n",
    "    \n",
    "    return {\n",
    "        'en': en_padded,\n",
    "        'de': de_padded,\n",
    "        'en_lengths': torch.tensor([len(seq) for seq in en_sequences]),\n",
    "        'de_lengths': torch.tensor([len(seq) for seq in de_sequences])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb4f1897-6842-4e56-a4f2-23c6bcf74e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataframe, vocab, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the translation dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: pandas DataFrame with tokenized sentences\n",
    "        vocab: shared vocabulary from BPE model\n",
    "        batch_size: size of batches\n",
    "        shuffle: whether to shuffle the data\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader object\n",
    "    \"\"\"\n",
    "    dataset = TranslationDataset(dataframe, vocab)\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda b: collate_batch(b, vocab)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb3e3202-80d6-4421-86f2-4994ff53a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(\n",
    "    df_encoded,\n",
    "    sb_vocab_dict,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09093dc0-b92a-422d-b074-62f430c0a4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': tensor([[    1, 36049,    58,  ...,     5,     5,     5],\n",
      "        [    1,  1898,   283,  ...,     5,     5,     5],\n",
      "        [    1, 11579, 20834,  ...,     5,     5,     5],\n",
      "        ...,\n",
      "        [    1,   290, 33479,  ...,     5,     5,     5],\n",
      "        [    1, 27870,  4955,  ...,     5,     5,     5],\n",
      "        [    1,   290, 33479,  ...,     5,     5,     5]]), 'de': tensor([[    1,  1083,  4202,  ...,     5,     5,     5],\n",
      "        [    1,  7404, 36940,  ...,     5,     5,     5],\n",
      "        [    1,  7404, 36940,  ...,     5,     5,     5],\n",
      "        ...,\n",
      "        [    1,  2374, 13618,  ...,     5,     5,     5],\n",
      "        [    1, 27870,  5202,  ...,  2281, 36927,     2],\n",
      "        [    1, 21913,  2983,  ...,     5,     5,     5]]), 'en_lengths': tensor([10, 26, 50, 57, 33, 23, 16, 30, 22, 25, 29, 18, 49, 23, 29, 17, 31, 25,\n",
      "        16, 11, 27, 25, 41, 39, 23, 17, 32, 25, 32, 54, 49, 23]), 'de_lengths': tensor([13, 22, 40, 47, 27, 29, 14, 44, 21, 24, 55, 20, 55, 24, 37, 22, 32, 23,\n",
      "        33,  9, 58, 15, 52, 25, 26, 19, 35, 28, 31, 42, 61, 34])}\n",
      "{'en': tensor([[    1, 29165, 23611,  ...,     5,     5,     5],\n",
      "        [    1, 21201,     9,  ...,     5,     5,     5],\n",
      "        [    1, 26069,    32,  ...,     5,     5,     5],\n",
      "        ...,\n",
      "        [    1,  1898,   283,  ...,     5,     5,     5],\n",
      "        [    1, 29165, 23611,  ...,     5,     5,     5],\n",
      "        [    1, 14464,  1866,  ...,     5,     5,     5]]), 'de': tensor([[    1,   720, 36968,  ...,     5,     5,     5],\n",
      "        [    1,  5363,   258,  ...,     5,     5,     5],\n",
      "        [    1,   414,    64,  ...,     5,     5,     5],\n",
      "        ...,\n",
      "        [    1,   415,   983,  ...,     5,     5,     5],\n",
      "        [    1,  5694, 36952,  ...,     5,     5,     5],\n",
      "        [    1,  3647,  1548,  ...,     5,     5,     5]]), 'en_lengths': tensor([40, 16, 39, 19, 27, 27, 42, 41, 47, 17, 47, 23, 58, 33, 64, 23, 34, 61,\n",
      "        55, 40, 41, 20, 25, 25, 36, 14, 34, 34, 16, 34, 42, 28]), 'de_lengths': tensor([35, 25, 52, 19, 46, 24, 53, 48, 38, 19, 62, 17, 73, 28, 56, 17, 30, 61,\n",
      "        56, 54, 23, 20, 38, 26, 42, 19, 36, 44, 20, 39, 48, 30])}\n",
      "{'en': tensor([[    1,  3815,  1617,  ...,     5,     5,     5],\n",
      "        [    1,   205,   226,  ...,     5,     5,     5],\n",
      "        [    1,  1898,   696,  ...,     5,     5,     5],\n",
      "        ...,\n",
      "        [    1, 21201,     9,  ...,     5,     5,     5],\n",
      "        [    1,    80,   802,  ...,     5,     5,     5],\n",
      "        [    1,  3267,    71,  ...,     5,     5,     5]]), 'de': tensor([[    1,   944,    62,  ...,     5,     5,     5],\n",
      "        [    1,   205,   796,  ...,     5,     5,     5],\n",
      "        [    1,  7404, 36940,  ...,     5,     5,     5],\n",
      "        ...,\n",
      "        [    1,  5363,   258,  ...,     5,     5,     5],\n",
      "        [    1, 27549,   258,  ...,  2158, 36927,     2],\n",
      "        [    1,  3215, 36979,  ...,     5,     5,     5]]), 'en_lengths': tensor([43, 30, 23, 32, 14, 40, 11, 37, 64, 32, 37, 38, 14, 16, 15, 23, 20,  9,\n",
      "        36, 22, 28, 21, 26, 43, 40, 20, 20, 45, 34, 15, 59, 16]), 'de_lengths': tensor([35, 30, 32, 36, 15, 42, 14, 26, 23, 31, 41, 40, 16, 29, 17, 25, 47, 11,\n",
      "        47, 23, 35, 23, 29, 40, 32, 35, 26, 53, 42, 27, 58, 12])}\n",
      "{'en': tensor([[    1,  4228,   280,   882,   583,    52,   763, 24877, 36926,    25,\n",
      "          5113,   283,   789,    58, 10489,   957,  7135, 36926,   499,   280,\n",
      "          6324, 36982, 36909,  1165,  1617,    59,    25, 15072,  9679,    52,\n",
      "            25, 14440, 36940, 16909,  6653,  3818, 36927,     2],\n",
      "        [    1,   773,  1968, 36926,   280,  1205,   264, 10475, 24877,    32,\n",
      "           473, 14440, 36940,   170,  1610,  1164, 34916, 36926,   299,   283,\n",
      "         13707,   310, 14295,    59, 14295, 36927,     2,     5,     5,     5,\n",
      "             5,     5,     5,     5,     5,     5,     5,     5],\n",
      "        [    1,  6017,   122,  4920, 11478, 36926,     9, 20558, 28470,   137,\n",
      "         19553,  3806,    58,    44,   102,  2216,   280,   182,  7165,    59,\n",
      "          3924,    58,   551,  8872, 36927,     2,     5,     5,     5,     5,\n",
      "             5,     5,     5,     5,     5,     5,     5,     5],\n",
      "        [    1,  4228,   280,   882, 14440, 36940, 36926,   280,   373, 11080,\n",
      "          1163, 24877,   130,   236,   776,    32, 27870,  1583,  3415,    59,\n",
      "          8365,     9,  7177,    91, 30062,   120,  2047, 14295, 36927,     2,\n",
      "             5,     5,     5,     5,     5,     5,     5,     5]]), 'de': tensor([[    1, 14440, 36940, 36943, 17030, 36926,    32,  1598,    73,  9642,\n",
      "            70,   307,    49,  1473,  4222,   166, 36926,  2281,   249,  8866,\n",
      "         18815, 34321,    65,   276,  2616,     6, 36926,   706,    62, 14531,\n",
      "         36943, 28512,  5834,    64, 18516, 14440, 36940, 36943,  1845,   346,\n",
      "           321,  1216,   239, 11552, 36927,     2],\n",
      "        [    1,   253,  2088,   258,   413,  4402, 14531,    32,  2791, 14440,\n",
      "         36940, 36943, 14383,   181,  2857,  4041,  5172,   908, 25952, 36926,\n",
      "            62,  1885,   459,   220, 10432, 24717, 16404,   259, 36927,     2,\n",
      "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "             5,     5,     5,     5,     5,     5],\n",
      "        [    1,  9476, 36972,  7370, 13498,  8850,  8458,  3882, 36927,     2,\n",
      "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "             5,     5,     5,     5,     5,     5],\n",
      "        [    1,  5363,   258,   181, 14440, 36940,  4382, 36926,   509,   258,\n",
      "           580, 14531,    32, 27870,    73, 36943,   393,   276,   107,  3520,\n",
      "         36926,  1475,   173,  4839, 10757,  7676,   219,  5411,    91, 30062,\n",
      "         36943, 14383,  2931,   451,   131,   740, 36927,     2,     5,     5,\n",
      "             5,     5,     5,     5,     5,     5]]), 'en_lengths': tensor([38, 27, 26, 30]), 'de_lengths': tensor([46, 30, 10, 38])}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "546f1bda-3bca-4849-8314-3754552f9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate positional encodings\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_mask(self, src, tgt):\n",
    "        # Create source padding mask\n",
    "        src_mask = src == 0  # Assuming 0 is the padding index\n",
    "        \n",
    "        # Create target padding mask\n",
    "        tgt_mask = tgt == 0\n",
    "        \n",
    "        # Create target subsequent mask (for autoregressive property)\n",
    "        seq_len = tgt.size(1)\n",
    "        subsequent_mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool()\n",
    "        subsequent_mask = subsequent_mask.to(tgt.device)\n",
    "        \n",
    "        return src_mask, tgt_mask, subsequent_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # Create masks\n",
    "        src_key_padding_mask, tgt_key_padding_mask, tgt_mask = self.create_mask(src, tgt)\n",
    "        \n",
    "        # Embed and add positional encoding\n",
    "        src_embedded = self.positional_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Transform\n",
    "        output = self.transformer(\n",
    "            src_embedded,\n",
    "            tgt_embedded,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd0ec416-6323-453f-901c-98b1788b0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, criterion, vocab_size, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model\n",
    "        val_dataloader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        vocab_size: Size of vocabulary\n",
    "        device: Device to evaluate on\n",
    "    \n",
    "    Returns:\n",
    "        Average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            src = batch['en'].to(device)\n",
    "            tgt = batch['de'].to(device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output.contiguous().view(-1, vocab_size), \n",
    "                           tgt_output.contiguous().view(-1))\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            non_pad_tokens = (tgt_output != 0).sum().item()\n",
    "            \n",
    "            total_loss += loss.item() * non_pad_tokens\n",
    "            total_tokens += non_pad_tokens\n",
    "    \n",
    "    return total_loss / total_tokens\n",
    "\n",
    "def train_transformer(model, train_dataloader, val_dataloader, vocab_size, num_epochs, \n",
    "                     save_path, save_interval, patience=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the transformer model with validation.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model\n",
    "        train_dataloader: DataLoader for training data\n",
    "        val_dataloader: DataLoader for validation data\n",
    "        vocab_size: Size of vocabulary\n",
    "        num_epochs: Number of training epochs\n",
    "        save_path: Path to save model checkpoints\n",
    "        save_interval: Save model every N iterations\n",
    "        patience: Number of epochs to wait for improvement before early stopping\n",
    "        device: Device to train on\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.044, betas=(0.9, 0.98), eps=1e-9)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    def lr_lambda(step):\n",
    "        warmup_steps = 4000\n",
    "        step = max(1, step)\n",
    "        return min(step ** (-0.5), step * warmup_steps ** (-1.5))\n",
    "    \n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # Training tracking\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "\n",
    "    \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            src = batch['en'].to(device)\n",
    "            tgt = batch['de'].to(device)\n",
    "            \n",
    "            # Shift target for teacher forcing\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output.contiguous().view(-1, vocab_size), \n",
    "                           tgt_output.contiguous().view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            # if global_step % save_interval == 0:\n",
    "            #     checkpoint = {\n",
    "            #         'epoch': epoch,\n",
    "            #         'global_step': global_step,\n",
    "            #         'model_state_dict': model.state_dict(),\n",
    "            #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #         'scheduler_state_dict': scheduler.state_dict(),\n",
    "            #         'loss': loss.item()\n",
    "            #     }\n",
    "            #     torch.save(checkpoint, f'{save_path}/checkpoint_{global_step}.pt')\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}, '\n",
    "                      f'Time: {elapsed:.2f}s, Learning Rate: {scheduler.get_last_lr()[0]:.7f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = evaluate(model, val_dataloader, criterion, vocab_size, device)\n",
    "        avg_train_loss = total_train_loss / num_train_batches\n",
    "        \n",
    "        # Log epoch results\n",
    "        print(f'Epoch {epoch} completed:')\n",
    "        print(f'  Average Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'  Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'train_loss': avg_train_loss\n",
    "            }, f'{save_path}/best_model.pt')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            model.load_state_dict(best_model_state)  # Restore best model\n",
    "            break\n",
    "    \n",
    "    return model, best_val_loss\n",
    "\n",
    "def create_train_val_dataloaders(dataset, batch_size, vocab, val_split=0.1, shuffle=True):\n",
    "    val_length = int(len(dataset) * val_split)\n",
    "    train_length = len(dataset) - val_length\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_length, val_length],\n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda b: collate_batch(b, vocab)\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # No need to shuffle validation data\n",
    "        collate_fn=lambda b: collate_batch(b, vocab)\n",
    "    )\n",
    "\n",
    "    print(len(train_dataloader))\n",
    "    print(len(val_dataloader))\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Example usage\n",
    "def create_and_train_transformer(dataset, vocab_size, vocab, save_path, batch_size=32, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create and train the transformer model with validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The full dataset\n",
    "        vocab_size: Size of vocabulary\n",
    "        save_path: Path to save model checkpoints\n",
    "        batch_size: Batch size for training\n",
    "        device: Device to train on\n",
    "    \"\"\"\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader, val_dataloader = create_train_val_dataloaders(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        vocab=vocab,\n",
    "        val_split=0.1\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, best_val_loss = train_transformer(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        vocab_size=vocab_size,\n",
    "        num_epochs=100,  # Adjust as needed\n",
    "        save_path=save_path,\n",
    "        save_interval=1000,  # Save every 1000 steps\n",
    "        patience=5,  # Early stopping patience\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return model, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f822de59-d3f3-49d4-8b9c-18c07677cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints'  # Make sure this directory exists\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1c4fc7b-9d74-4f0a-b1c0-02cf03e1f0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "10\n",
      "Epoch: 0, Batch: 0, Loss: 10.4878, Time: 1.37s, Learning Rate: 0.0000002\n",
      "Epoch 0 completed:\n",
      "  Average Train Loss: 10.3666\n",
      "  Validation Loss: 10.1594\n",
      "Epoch: 1, Batch: 0, Loss: 10.0400, Time: 97.84s, Learning Rate: 0.0000158\n",
      "Epoch 1 completed:\n",
      "  Average Train Loss: 9.6079\n",
      "  Validation Loss: 9.4577\n",
      "Epoch: 2, Batch: 0, Loss: 8.9916, Time: 178.58s, Learning Rate: 0.0000315\n",
      "Epoch 2 completed:\n",
      "  Average Train Loss: 8.2168\n",
      "  Validation Loss: 8.6608\n",
      "Epoch: 3, Batch: 0, Loss: 7.2922, Time: 257.63s, Learning Rate: 0.0000471\n",
      "Epoch 3 completed:\n",
      "  Average Train Loss: 7.0242\n",
      "  Validation Loss: 8.6421\n",
      "Epoch: 4, Batch: 0, Loss: 6.8194, Time: 336.36s, Learning Rate: 0.0000628\n",
      "Epoch 4 completed:\n",
      "  Average Train Loss: 6.6230\n",
      "  Validation Loss: 8.9947\n",
      "Epoch: 5, Batch: 0, Loss: 6.5465, Time: 411.36s, Learning Rate: 0.0000784\n",
      "Epoch 5 completed:\n",
      "  Average Train Loss: 6.5254\n",
      "  Validation Loss: 9.3673\n",
      "Epoch: 6, Batch: 0, Loss: 6.0017, Time: 486.78s, Learning Rate: 0.0000941\n",
      "Epoch 6 completed:\n",
      "  Average Train Loss: 6.5109\n",
      "  Validation Loss: 9.5791\n",
      "Epoch: 7, Batch: 0, Loss: 6.4454, Time: 562.30s, Learning Rate: 0.0001097\n",
      "Epoch 7 completed:\n",
      "  Average Train Loss: 6.4964\n",
      "  Validation Loss: 10.1459\n",
      "Epoch: 8, Batch: 0, Loss: 6.3083, Time: 637.65s, Learning Rate: 0.0001254\n",
      "Epoch 8 completed:\n",
      "  Average Train Loss: 6.5543\n",
      "  Validation Loss: 10.2825\n",
      "Early stopping triggered after 9 epochs\n"
     ]
    }
   ],
   "source": [
    "dataset = TranslationDataset(df_encoded, sb_vocab)\n",
    "\n",
    "model, best_val_loss = create_and_train_transformer(\n",
    "    dataset=dataset,  # Your full dataset\n",
    "    vocab_size=vocab_size,\n",
    "    vocab=sb_vocab_dict,\n",
    "    save_path=save_path,\n",
    "    batch_size=1,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c0e75-eafe-4430-aaae-5bafccb9e87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7c895-7784-49d7-83b9-d50ef596dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_model.model')\n",
    "\n",
    "# Create a batch of encoded sequences\n",
    "encoded_en, encoded_de = create_sample_batch(df_normalized, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb1c82-79a9-436a-ae5d-b98f1c35f36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
